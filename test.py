from time import time
st = time()
import numpy as np
import librosa
from train import get_networks, prep, deprep
import os
import soundfile as sf
import matplotlib.pyplot as plt
from glob import glob
import sounddevice as sd
from librosa.display import specshow
from config import Config

para = Config()
hop = para.hop               #hop size (window size = 6*hop)
sr = para.sr              #sampling rate
min_level_db = para.min_level_db     #reference values to normalize data
ref_level_db = para.ref_level_db
shape = para.shape              #length of time axis of split specrograms to feed to generator            
vec_len = para.vec_len           #length of vector generated by siamese vector
bs = para.bs                #batch size
delta = para.delta            #constant for siamese loss


print(f'loading packages: {time()-st}sec')
# Load trained model
st = time()
model_path = 'models/' + 'MELGANVC-male1_to_female1'
gen,critic,siam, [opt_gen,opt_disc] = get_networks(shape, load_model=True, path= model_path)


# Generally, loading time is 0.88 secs
print(f'loading model: {time()-st}sec')
#After Training, use these functions to convert data with the generator and save the results

#Assembling generated Spectrogram chunks into final Spectrogram
def specass(a,spec):
  but=False
  con = np.array([])
  nim = a.shape[0]
  for i in range(nim-1):
    im = a[i]
    im = np.squeeze(im)
    if not but:
      con=im
      but=True
    else:
      con = np.concatenate((con,im), axis=1)
  diff = spec.shape[1]-(nim*shape)
  a = np.squeeze(a)
  con = np.concatenate((con,a[-1,:,-diff:]), axis=1)
  return np.squeeze(con)

#Splitting input spectrogram into different chunks to feed to the generator
def chopspec(spec):
  dsa = []
  for i in range(spec.shape[1]//shape):
    im = spec[:,i*shape:i*shape+shape]
    im = np.reshape(im, (im.shape[0],im.shape[1],1))
    dsa.append(im)
  imlast = spec[:,-shape:]
  imlast = np.reshape(imlast, (imlast.shape[0],imlast.shape[1],1))
  dsa.append(imlast)
  return np.array(dsa, dtype=np.float32)

#Converting from source Spectrogram to target Spectrogram
def towave(spec, name, path, show=False,save=True):
  specarr = chopspec(spec)
  a = specarr
  #print('Generating WAV...')
  ab = gen(a, training=False)
  ab = specass(ab,spec)
  abwv = deprep(ab)
  if save:
    pathfin = f'{path}/{name}'
    if not os.path.isdir(pathfin):
      os.mkdir(pathfin)
    sf.write(pathfin+'/AB.wav', abwv, sr)
    print('Saved WAV!')
  if show:
    plot_mels(a,ab)
  return abwv

#Wav to wav conversion

def plot_mel(spec):
    fig, ax = plt.subplots(figsize=(16, 9), sharey=True)
    img = specshow(spec,y_axis='mel',x_axis='time',ax=ax)
    ax.set(title='mel-spectrogram')
    fig.colorbar(img, ax=ax, format="%+2.2f dB")
    plt.show()
    
def plot_mels(spec,abspec):
    fig, ax = plt.subplots(2,1,figsize=(16, 9), sharey=True)
    img0 = specshow(spec,y_axis='mel',x_axis='time',ax=ax[0])
    img1 = specshow(abspec,y_axis='mel',x_axis='time',ax=ax[1])   
    ax[0].set(title='original mel-spectrogram')
    ax[1].set(title='converted mel-spectrogram')
    fig.colorbar(img0, ax=ax[0], format="%+2.2f dB")
    fig.colorbar(img1, ax=ax[1], format="%+2.2f dB")
    plt.show()  

def plot_waves(wv,abwv):
    fig, ax = plt.subplots(2,1,figsize=(16, 9), sharey=True)
    ax[0].set(title='original speech')
    ax[0].plot(wv)
    ax[1].set(title='converted speech')
    ax[1].plot(abwv)
    plt.show()
    
def voice_conversion(wv):
  '''
    conduct voice conversion
    Parameters
    ----------
    wv : ndarray
        The source voice to be converted.

    Returns
    -------
    wv : ndarray
        The target voice.
        
    '''
  original_shape = wv.shape

  if abs(np.max(wv)) > 0.002:
      wv = librosa.util.normalize(wv)
  speca = prep(wv)
  wv = towave(speca, name='zsl_to_female2', path='/results',save=False)
  if original_shape[0]-wv.shape[0] > 0:
      wv = np.expand_dims(np.pad(wv,((0,original_shape[0]-wv.shape[0]))), axis=1)
  else:
      wv = np.expand_dims(wv[:original_shape[0]],axis=1)
  return wv


if __name__ == '__main__':
  num_of_test = 10
  wav_list = glob('dataset/test/zsl/*.wav')[:num_of_test] #one should change directory if he/she wants to using different data
  #wav_list = glob('dataset/male2_rms/wav/*.wav')[:num_of_test]
  
  for i,wav_file in enumerate(wav_list):
    st = time()
    fn = wav_file.split('\\')[-1]
    print(f'processing file:{fn}')
    wv, sr = librosa.load(wav_file, sr=para.sr)  #Load target waveform
    abwv = voice_conversion(wv)
    plot_waves(wv,abwv)
    plot_mels(prep(wv),prep(abwv))
    sd.play(abwv,16000)
    input()

